# Buffer Poll 底层（InnoDB）

## 缓存简述

InnoDB 存储引擎在处理客户端的请求时，如果要访问某个页的数据（一个页的一条记录也一样），就会把完整的页的数据全部加载到内存中。将整个页加载到内存中后就可以进行读写访问，当进行完读写访问之后，不会立即把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘IO 的开销。

------

## InnoDB的Buffer Pool

### Buffer Pool

为了缓存磁盘中的页，在MySQL 服务器启动的时候就向操作系统申请了**一片连续的内存**，这片内存就叫做`Buffer Pool` (缓冲池)。

默认情况下Buffer Pool 只有128M 大小。也可以在启动服务器的时候配置innodb_buffer_pool_size 参数的值，它表示Buffer Pool 的大小：

```ini
[server]
innodb_buffer_pool_size = 268435456
```

其中， 268435456 的单位是字节，也就是我指定Buffer Pool 的大小为256M 。

### Buffer Pool 内部组成

Buffer Pool对应的一片连续的内存被划分为若干个页面，页面大小与InnoDB表空间使用的页面大小一致，默认16KB。缓冲页除了拥有一个页的通用信息，还有一些别的控制信息。

每个缓存页对应的控制信息占用的内存大小是相同的，把每个页对应的控制信息占用的一块内存称为一个**控制块**。控制块和缓存页一一对应，都存放在 Buffer Pool 中，其中控制块被存放到 Buffer Pool的前边，缓存页被存放到Buffer Pool 后边。

![image-20211116160811077](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/53.png)

其中，用不到的内存空间就被称为碎片，如果 Buffer Pool 的大小设置的刚好，就没有锁片。每个控制块大约占用缓存页大小的5%。

### Buffer Pool 初始化过程

先向操作系统申请Buffer Pool 的内存空间，然后把它划分成若干对控制块和缓存页。此时并没有真实的磁盘页被缓存到Buffer Pool 中（因为还没有用到），之后随着程序的运行，会不断的有磁盘上的页被缓存到Buffer Pool 中。

### free链表

从磁盘上读取一个页到Buffer Pool 中的时候该放到哪个缓存页的位置呢？或者说怎么区分Buffer Pool 中哪些缓存页是空闲的，哪些已经被使用了呢？——> free链表

free链表（空闲链表）：把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表就是free链表。用来记录Buffer Pool中哪些缓存页是可用的。

![image-20211116162122460](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/54.png)

基节点：用来管理free链表，包含着链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。

链表的基节点占用的内存空间并不包含在为Buffer Pool 申请的一大片连续内存空间之内，而是单独申请的一块内存空间。

每当需要从磁盘中加载一个页到Buffer Pool 中时，就从free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上（就是该页所在的表空间、页号之类的信息），然后把该缓存页对应的free链表节点从链表中移除，表示该缓存页已经被使用——> 取页、填信息、节点移除

### 缓存页的哈希处理

怎么知道一个页在不在Buffer Pool 中呢？需要依次遍历Buffer Pool 中各个缓存页么？——> 哈希处理

根据表 空间号 + 页号 来定位一个页的，相当于 表空间号 + 页号 是一个key ，缓存页 就是对应的value

可以用 表空间号 + 页号 作为key ， 缓存页 作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据 表空间号 + 页号 看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。

### flush链表

脏页： Buffer Pool 中某个缓存页的数据被修改，导致和磁盘上的页不一致，这种还没刷入磁盘的的缓存页叫做脏页（ dirty page ）。

脏页刷回磁盘并不是立刻执行的，而是在未来的某个时间点进行同步。不立即同步到磁盘的话，之后再同步的时候就需要通过flush链表知道Buffer Pool 中哪些页是脏页。

flush链表：一个存储脏页的链表，还没刷回磁盘的缓存页对应的控制块都会作为一个节点加入到一个链表中。

flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。

![image-20211116164416419](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/55.png)

### LRU链表

需要缓存的页占用的内存大小超过了Buffer Pool 大小，free链表中已经没有多余的空闲缓存页的时候，需要把某些旧的缓存页从Buffer Pool 中移除，然后再把新的页放进来。移除旧的缓存页，需要通过buffer poll缓存命中率和LRU链表来完成。

buffer poll缓存命中率：假设一共访问了n 次页，那么被访问的页已经在缓存中的次数除以n就是所谓的缓存命中率。缓存命中率越高越好。

#### 简单的LRU链表

**按照最近最少使用的原则淘汰缓存页形成的链表，称为LRU链表**（Least Recently Used）。简单来说，使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页

#### 划分区域的LRU链表

预读：InnoDB 认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。很多预读的页面都没有用到的话，会大大降低缓存命中率。

可能降低Buffer Pool命中率的两种情况：

- 加载到Buffer Pool 中的页不一定被用到——预读导致
- 如果非常多的使用频率偏低的页被同时加载到Buffer Pool 时，可能会把那些使用频率非常高的页从Buffer Pool 中淘汰掉。——全表扫描导致

面对这两种情况，LRU链表按照**一定比例**分成两截：

- 热数据（young区域）：存储使用频率非常高的缓存页。
- 冷数据（old区域）：存储使用频率不是很高的缓存页。

![image-20211116173449614](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/56.png)

查看old区域在LRU链表所占比例：

```sql
SHOW VARIABLES LIKE 'innodb_old_blocks_pct';

+-----------------------+-------+
| Variable_name | Value |
+-----------------------+-------+
| innodb_old_blocks_pct | 37 |
+-----------------------+-------+
```

从结果可以看出来，默认情况下， old 区域在LRU链表中所占的比例是37%。

可以修改innodb_old_blocks_pct 参数来控制old 区域在LRU链表中所占的比例：

```ini
; 启动时设置
[server]
innodb_old_blocks_pct = 40
; 运行中修改
SET GLOBAL innodb_old_blocks_pct = 40;
```

##### 划分区域的LRU链表运行原理

LRU链表分为young区域和old区域，可以通过`innodb_old_blocks_pct`来调节old区域所占比例。首次从磁盘加载到Buffer pool中的页会放到old区域的头部，在`innodb_old_blocks_time`间隔时间内访问该页时，不会把它移动到young区域头部（可能是一张页里多条记录后才找到我们想要的记录）。在Buffer Pool中没有可用的空闲缓冲页时，会首先淘汰掉old区域的一些页。

#### 进一步优化LRU链表

频繁的对LRU链表执行节点移动操作十分消耗性能，可以采用类似：被访问的缓冲页位于young区域的1/4时，才会被移动到LRU链表头部。

### 刷新脏页到磁盘

后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径：

- 从LRU链表的冷数据中刷新一部分页面到磁盘。

后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量 `innodb_lru_scan_depth` 来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为`BUF_FLUSH_LRU` 。

- 从flush链表中刷新一部分页面到磁盘。

后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是否繁忙。这种刷新页面的方式被称之为`BUF_FLUSH_LIST` 。

### 多个Buffer Pool实例

多线程并发访问特别高的情况下，单一的 Buffer Pool 可能会影响请求的处理速度。在Buffer Pool 特别大的时候，会把Buffer Pool拆分成若干个小的Buffer Pool ，每个Buffer Pool 都称为一个实例，每个实例相互独立，独立的去申请内存空间，从而提高并发处理能力。

![image-20211116220312961](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/57.png)

修改Buffer Pool 实例的个数：

```sql
[server]
innodb_buffer_pool_instances = 2
```

当innodb_buffer_pool_size的值小于1G的时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances 的值修改为1。只有大于1G的时候设置才生效。

一个Buffer Pool实例其实是由若干个chunk 组成的，一个chunk 就代表一片连续的内存空间，里面包含了若干缓存页与其对应的控制块。

![image-20211116220744115](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/58.png)

在服务器运行期间调整Buffer Pool的大小时，就是以chunk 为单位增加或者删除内存空间，而不需要重新向操作系统申请一片大的内存，然后进行缓存页的复制。

**查看buffer pool的状态信息**：

```sql
SHOW ENGINE INNODB STATUS\G
```

该指令可以查看关于InnoDB 存储引擎运行过程中的一些状态信息，其中就包括Buffer Pool 的一些信息。

------

------

# 日志

## 简述

在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的方方面面，以帮助数据库管理员追踪数据库曾经发生过的各种事件。在 MySQL 中，有7种不同的日志，分别是：

- 重做日志（redo log）
- 回滚日志（undo log）
- 二进制日志（binlog），也叫归档日志
- 错误日志（errorlog）
- 慢查询日志（slow query log）
- 一般查询日志（general log）
- 中继日志（relay log）

其中，重做日志、回滚日志和二进制日志与事务操作、数据库恢复等操作息息相关。

------

## redo log（重做日志）

### 基本介绍

#### 为什么需要redo log？

数据的**持久化**需要把内存里的页面刷回磁盘，如果在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但这样会有一些问题：

- 有可能仅修改了某个页的一条记录，刷新一个完整的数据页太浪费空间；
- 随机I/O刷新比较慢

所以，可以只把修改的地方记录一下就好，于是就有了redo log。

#### redo log简介

**Redo log**：重做日志，redo日志会把事务在执行过程中对数据库所做的所有修改都记录下来，在之后系统奔溃重启后可以把事务所做的任何修改都恢复出来。redo log占用的空间非常小，并且是按顺序写入磁盘。

redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的redo日志文件(redo log file)，该部分日志是持久的。

在概念上，innodb通过**force log at commit**机制实现事务的持久性，即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的redo log file和undo log file中进行持久化。

从redo log buffer写日志到磁盘的redo log file中过程：

![](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/59.png)

为了确保每次日志都能写入到事务日志文件中，在每次将log buffer中的日志写入日志文件的过程中都会调用一次操作系统的fsync操作（即fsync()系统调用）。因为MariaDB / MySQL是工作在用户空间的，MariaDB / MySQL的log buffer处于用户空间的内存中。要写入到磁盘上的log file中（redo：ib_logfileN文件，undo：share tablespace 或 .ibd文件），中间还要经过操作系统内核空间的os buffer，调用fsync()的作用就是将OS buffer中的日志刷到磁盘上的log file中。

注意，一般所说的log file并不是磁盘上的物理日志文件，而是操作系统缓存中的log file，但本文中log file则表示磁盘上的物理日志文件，即log file on disk。

MySQL支持用户自定义在commit时，如何将log buffer中的日志刷log file中。系统变量`innodb_flush_log_at_trx_commit`控制着在事务提交时，是否将该事务运行过程中产生的redo刷新到磁盘。该变量有3种值：0、1、2，默认为1。但注意，这个变量只是控制commit动作是否刷新log buffer到磁盘。

- 当设置为1的时候，事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file on disk中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。
- 当设置为0的时候，事务提交时不会将log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到log file on disk中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。
- 当设置为2的时候，每次提交都仅写入到os buffer，然后是每秒调用fsync()将os buffer中的日志写入到log file on disk。

![](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/60.png)

注意，有一个变量 innodb_flush_log_at_timeout 的值为1秒，该变量表示的是刷日志的频率。

在主从复制结构中，要保证事务的持久性和一致性，需要对日志相关变量设置为如下：

- **如果启用了二进制日志，则设置sync_binlog=1，即每提交一次事务同步写到磁盘中。**
- **总是设置innodb_flush_log_at_trx_commit=1，即每提交一次事务都写到磁盘中。**

上述两项变量的设置保证了：每次提交事务都写入二进制日志和事务日志，并在提交时将它们刷新到磁盘中。

------

### redo日志格式

大部分类型的redo日志采用这种通用的结构：

![image-20211117212022741](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/61.png)

- type ：该条redo 日志的类型

  在MySQL 5.7.21 这个版本中，redo 日志一共有53种不同的类型

- space ID ：表空间ID

- page number ：页号

- data ：该条redo 日志的具体内容

------

### Mini-Transaction

- 以组的形式写入redo日志

在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统奔溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。

> 举例：
>
> 一个原子性操作，比如，插一条记录，可能会导致页分裂等很多种连锁反应发生，所以可能需要写入很多个redo log日志，这样一条插入记录相关的redo log需要按组写入redo log中。不然在数据库发生故障恢复的时候，无法保证能过恢复该事务的效果。

- Mini-Transaction的概念

**把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction （MTR）**。一次MTR 可以包含一组redo日志，在进行奔溃恢复时这一组redo日志作为一个不可分割的整体。

一个事务可以包含若干条语句，每一条语句其实是由若干个MTR组成，每一个MTR又可以包含若干条redo日志，它们的关系：

![image-2021111721202](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/62.png)

------

### redo日志写入过程

#### redo log block（日志块）

把通过MTR生成的redo日志都放在了大小为512字节的页中，把用来存储redo日志的页称为block。

每个redo log block由3部分组成：**日志块头、日志块尾和日志主体**。其中日志块头占用12字节，日志块尾占用8字节，每个redo log block的日志主体占用496字节。

![image-20211117215103982](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/63.png)

真正的redo日志都是存储到占用496字节大小的log block body中，log block header和log block trailer 存储的是一些管理信息。因为redo log记录的是数据页的变化，当一个数据页产生的变化需要使用超过496字节的redo log来记录，那么就会使用多个redo log block来记录该数据页的变化。

几个属性的含义：

- log_block_hdr_no：该日志块在redo log buffer中的位置ID。
- log_block_hdr_data_len：该log block中已记录的log大小。写满该log block时为0x200，表示512字节。
- log_block_first_rec_group：该block中第一个MTR生成的第一条redo日志的偏移量。

一条redo日志也可以称之为一条redo日志记录（ redo log record ），一个MTR会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（ redo log record group ）。

- log_block_checkpoint_no：写入检查点信息的位置。

- log_block_checksum ：表示block的校验值，用于正确性校验。

#### redo日志缓冲区

同buffer pool一样，为了解决磁盘速度过慢的问题，写入redo日志也不是直接直接写到磁盘上。在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，称为redo日志缓冲区，也可以简称为log buffer。log buffer被划分成若干个连续的redo log block：

![image-20211117221653500](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/64.png)

**可以通过启动参数`innodb_log_buffer_size`来指定log buffer的大小**，在MySQL 5.7.21 这个版本中，该启动参数的默认值为16MB 。

#### redo日志写入log buffer

向log buffer中写入redo日志的过程是顺序的，先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。每次写入的起始点从buf_free的全局变量得知，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置。

![image-20211117221825501](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/65.png)

> 上图说明：
>
> 事务T1 的两个mtr 分别称为mtr_T1_1 和mtr_T1_2 。
> 事务T2 的两个mtr 分别称为mtr_T2_1 和mtr_T2_2 。
>
> 不同的mtr 产生的一组redo 日志占用的存储空间可能不一样，有的mtr 产生的redo 日志量很少，有的mtr 产生的redo 日志量非常大。

------

### redo日志文件

#### redo日志刷盘时机

- log buffer 空间不足时：如果当前写入log buffer中的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。
- 事务提交时：在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo 日志刷新到磁盘。（**两阶段提交**）
- 刷新脏页到磁盘之前，需要先把redo日志刷新到磁盘中。
- 后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。
- 正常关闭服务器时。
- 做checkpoint时（后面讲）。

#### redo日志文件组

MySQL 的数据目录（使用`SHOW VARIABLES LIKE 'datadir'` 查看）下默认有两个名为`ib_logfile0`和`ib_logfile1`的文件， log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。

通过启动参数来调节文件位置：

```sql
-- 该参数指定了redo 日志文件所在的目录，默认值就是当前的数据目录
innodb_log_group_home_dir

-- 该参数指定了每个redo 日志文件的大小，在MySQL 5.7.21 这个版本中的默认值为48MB
innodb_log_file_size

-- 该参数指定redo 日志文件的个数，默认值为2，最大值为100
innodb_log_files_in_group
```

磁盘上的redo 日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字] （ 数字可以是0 、1 、2 ...）的形式进行命名。按编号从小往大写。如果写到最后一个文件，那就又开始从ib_logfile0继续写。过程如下：

![image-20211117223538800](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/66.png)

总共的redo日志文件大小其实就是： innodb_log_file_size × innodb_log_files_in_group 。

#### redo日志文件格式

log buffer本质上是一片连续的内存空间，被划分成了若干个512 字节大小的block 。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512 字节大小的block组成。

redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成：

- 前4个block（2048个字节）用来存储一些管理信息。
- 往后用来存储log buffer中的block镜像。

循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算：

![image-20211117224007122](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/67.png)

图中每一个格子，就是一个block。

黄色的block跟普通的block一样，蓝色的block格式如下：

![image-20211118143844028](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/68.png)

- log file header ：描述该redo 日志文件的一些整体属性
- checkpoint1 ：记录关于checkpoint 的一些属性
- 第三个block未使用
- checkpoint2 ：结构和checkpoint1 一样

log file header属性介绍：

|           属性           |                             描述                             |
| :----------------------: | :----------------------------------------------------------: |
|    LOG_HEADER_FORMAT     |                       redo 日志的版本                        |
|     LOG_HEADER_PAD1      |                         用于字节填充                         |
| **LOG_HEADER_START_LSN** | 标记本redo 日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值 |
|    LOG_HEADER_CREATOR    |        一个字符串，标记本redo 日志文件的创建者是谁。         |
|    LOG_BLOCK_CHECKSUM    |                本block的校验值，所有block都有                |

checkpoint属性介绍：

|            属性             |                             描述                             |
| :-------------------------: | :----------------------------------------------------------: |
|    **LOG_CHECKPOINT_NO**    | 服务器执行checkpoint的编号，每做一次checkpoint ，该值就加1。 |
|   **LOG_CHECKPOINT_LSN**    | 服务器做checkpoint结束时对应的LSN 值，系统奔溃恢复时将从该值开始。 |
|    LOG_CHECKPOINT_OFFSET    |         上个属性中的LSN值在redo 日志文件组中的偏移量         |
| LOG_CHECKPOINT_LOG_BUF_SIZE |       服务器在做checkpoint操作时对应的log buffer的大小       |
|     LOG_BLOCK_CHECKSUM      |                本block的校验值，所有block都有                |

系统中checkpoint的相关信息只存储在redo日志文件组的第一个日志文件中。

------

#### Log Sequeue Number

LSN称为日志的逻辑序列号(log sequence number)，用来记录当前总共已经写入的redo日志量。在innodb存储引擎中，lsn占用8个字节，LSN的值会随着日志的写入而逐渐增大。

LSN值并不是对应一条日志，而是每一组由**MTR生成的redo日志**都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。

##### flushed_to_disk_lsn

redo日志是首先写到log buffer中，之后被刷新到磁盘上的redo日志文件。

全局变量`buf_next_to_write`：**标记当前log buffer中已经有哪些日志被刷新到磁盘中**。

全局变量`flushed_to_disk_lsn`：**表示刷新到磁盘中的redo 日志量**。

![image-20211118150035899](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/69.png)

当有新的redo日志写入到log buffer时，首先lsn的值会增长，flushed_to_disk_lsn不变，随后随着不断有log buffer 中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。**如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中**。

#### flush链表中的LSN

一个MTR代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在MTR结束时，**要把这一组redo日志写入到log buffer中，还要把在MTR执行过程中可能修改过的页面加入到Buffer Pool的flush链表**。

flush链表的被修改页对应的控制块中会记录两个关于页面何时修改的属性：

- oldest_modification ：如果某个页面被加载到Buffer Pool后进行第一次修改，将修改该页面的MTR开始时对应的lsn值写入这个属性。**凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的**。
- newest_modification ：每修改一次页面，都会将修改该页面的MTR结束时对应的lsn值写入这个属性。该属性表示页面最近一次修改后对应的系统lsn值。

![image-20211118151846360](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/70.png)

**flush链表中的脏页按照修改发生的时间顺序进行排序**，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。

------

### checkpoint

redo日志文件组中的文件是循环使用的，也就是redo日志是采用循环写的方式，这会造成最后写的redo日志与最开始写的redo日志追尾。**判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里**。

全局变量`checkpoint_lsn`：代表当前系统中**可以**被覆盖的redo日志总量是多少。

执行一次`checkpoint`：某个脏页被刷新到磁盘上，然后增加`checkpoint_lsn`的值，并把相关的信息存放到日志文件的管理信息（checkpoint1或者checkpoint2）中。

做一次checkpoint 其实可以分为两个步骤：

- 计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少
- 将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint 的编号写到日志文件的管理信息中。

### 和redo log有关的几个变量

- innodb_flush_log_at_trx_commit={0|1|2} # 指定何时将事务日志刷到磁盘，默认为1。
  - 0表示每秒将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。
  - 1表示每事务提交都将"log buffer"同步到"os buffer"且从"os buffer"刷到磁盘日志文件中。
  - 2表示每事务提交都将"log buffer"同步到"os buffer"但每秒才从"os buffer"刷到磁盘日志文件中。
- innodb_log_buffer_size：# log buffer的大小，默认8M
- innodb_log_file_size：#事务日志的大小，默认5M
- innodb_log_files_group =2：# 事务日志组中的事务日志文件个数，默认2个
- innodb_log_group_home_dir =./：# 事务日志组路径，当前目录表示数据目录
- innodb_mirrored_log_groups =1：# 指定事务日志组的镜像组个数，但镜像功能好像是强制关闭的，所以只有一个log group。在MySQL5.7中该变量已经移除。

参考：

[详细分析MySQL事务日志(redo log和undo log) - 骏马金龙 - 博客园 (cnblogs.com)](https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html#auto_id_0)

[MySQL中的几种日志了解 - myseries - 博客园 (cnblogs.com)](https://www.cnblogs.com/myseries/p/10728533.html)

------

------

## undo log（回滚日志）

undo log：记录一条记录的变化过程。undo log一般是逻辑日志，用来回滚行记录到某个版本。

作用：提供回滚和多个行版本控制(MVCC)。

**回滚**：为了保证事务的原子性，事务执行过程中遇到错误或者在事务执行时手动输入rollback结束当前事务，导致事务只执行到一半，需要撤销刚才事务里的操作，改回原来的样子，这个过程就叫回滚（rollback）。

MVCC内容在后面

------

### 事务id（trx_id）

- **分配事务id的时机**：只有在事务**对表中的记录进行改动时**才会为这个事务分配一个唯一的事务id。
  - 只读事务（START TRANSACTION READ ONLY）：对临时表做增、删、改操作时分配；
  - 读写事务（START TRANSACTION READ WRITE、BEGIN 、START TRANSACTION）：第一次对某个表（普通表、读写表）执行增、删、改时分配。
- **事务id的生成**：服务器会在内存中维护一个全局变量，该变量自增1，当事务需要事务id的时候就分配给它；每当该变量的值为256 的倍数时，就会刷新到系统表空间的页号为5 页面中的Max Trx ID 的属性里；当系统下一次重新启动，会把Max Trx ID的属性加载到内存，将Max Trx ID的值加上256之后赋值给全局变量。
- **trx_id隐藏列**：就是对聚簇索引记录做改动的语句所在事务对应的事务id。
- **roll pointer隐藏列**：本质上就是一个指向记录对应的undo日志的一个指针。

![image-20211029105324208](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/71.png)

**主键自动生成**：一条记录的行格式如上，聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。

需要注意，只有聚簇索引记录才有trx_id、roll_pointer这些属性，**修改二级索引的记录**时，只会影响二级索引记录所在页面的Page Header中PAGE_MAX_TRX_ID属性。

------

### undo日志格式

为了实现事务的原子性， InnoDB 存储引擎在实际进行增、删、改一条记录时，都需要先把对应的undo日志记下来。一般每对一条**记录**做一次改动，就对应着一条或两条undo日志。而一个事务在执行过程中，可能有多条记录，所以就需要对日志进行编号（**undo no**），undo no是一个自增1的字段。

这些undo日志是被记录到类型为FIL_PAGE_UNDO_LOG的页面中。这些页面可以从系统表空间中分配，也可以从一种专门存放undo日志的表空间（也就是所谓的undo tablespace 中分配）。

不同操作可能会产生不同格式的undo日志。

#### INSERT操作

如果回滚插入操作，那么需要把插入记录删除。写INSERT操作对应的undo日志时，主要是把这条记录的**主键各列信息**记上。TRX_UNDO_INSERT_REC 的 undo日志的格式：

![image-20211029110520614](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/72.png)

> 举例：插入一条记录 id是int型 
>
> ```sql
> INSERT INTO undo_demo(id, key1, col) VALUES (1, 'AWM', '狙击枪');
> ```
>

**主键各列信息**：len 就代表列占用的存储空间大小（比如，int类型占用4）， value 就代表列的真实值（主键id的值）。最后存入<4, 1>。

前面说到，roll pointer隐藏列，本质上就是一个指向记录对应的undo日志的一个指针。

![image-20211029111504004](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/73.png)

#### DELETE操作

##### 删除记录的过程

插入到页面中的记录会根据记录头信息中的next_record 属性组成一个单向链表，这个链表称之为**正常记录链表**；

被删除的记录其实也会根据记录头信息中的next_record 属性组成一个链表，称之为**垃圾链表**，这个链表中的记录占用的存储空间可以被重新利用；

**Page Header**部分有一个称之为**PAGE_FREE**的属性，它指向由被删除记录组成的垃圾链表中的头节点。

删除过程：

- **delete mark阶段**：将记录的**delete_mask**标识位设置为1 ，其他的不做修改（只会修改记录的trx_id 、roll_pointer 这些隐藏列的值）。
- **purge阶段**：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后调整一些页面的其他信息，比如页面中的用户记录数量PAGE_N_RECS 、上次插入记录的位置PAGE_LAST_INSERT等等。

> 举例：
>
> 未删除的样子：
>
> ![image-209111504004](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/74.png)
>
> 页面的Page Header部分的PAGE_FREE属性的值代表指向垃圾链表头节点的指针。
>
> delete mark阶段：
>
> ![image-2029111504004](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/75.png)
>
> 在删除语句所在的事务提交之前，只会经历delete mark阶段。
>
> purge阶段：
>
> ![image-202110504004](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/76.png)
>
> 将被删除记录加入到垃圾链表时，实际上加入到链表的头节点处，会跟着修改PAGE_FREE 属性的值。

##### TRX_UNDO_DEL_MARK_REC 类型的undo日志

![image-20211029140930385](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/77.png)

- **old trx_id和old roll_pointer**：对一条记录进行delete mark操作前，把该记录的trx_id 和roll_pointer隐藏列的**旧值**记录到对应的undo日志的trx_id 和roll_pointer属性中。
- **版本链**：不同的undo日志，通过**old roll_pointer**指针串成的链表。

> 举例：比如删除一条记录
>
> ![image-20211119210207768](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/78.png)
>
> 执行完delete mark 操作后，中间状态记录、delete mark 操作产生的undo 日志以及INSERT 操作后产生的undo 日志串成了链表。

- **<pos, len, value>索引列信息**：如果某个列被包含在某个索引中，那么它的相关信息就被记录到<pos, len, value>，该属性包括该列在记录中的位置（pos ），该列占用的存储空间大小（len ），该列实际值（value）。这部分信息主要是用在事务提交后，也就是purge 阶段中使用的。

#### UPDATE操作

在执行UPDATE 语句时， InnoDB 对更新主键和不更新主键这两种情况有有不同的undo日志。

##### 不更新主键

在不更新记录主键值时，先真正删除旧记录，再插入新记录的方式。分为：

- 就地更新（in-place update）：更新记录时，每个列在更新前后占用的存储空间**一样大**，就可以进行就地更新。
- 先删除掉旧记录，再插入新记录：在**不更新主键的情况下**，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，就采用这种方式。如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间，否则的话需要在页面中新申请一段空间以供新记录使用

针对这种情况，设计了一种类型为TRX_UNDO_UPD_EXIST_REC 的undo日志：

![image-20211029161213716](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/79.png)

n_updated属性：表示本条UPDATE 语句执行后将有几个列被更新。后边跟着的<pos, old_len, old_value>分别表示被更新列在记录中的位置、更新前该列占用的存储空间大小、更新前该列的真实值。

##### 更新主键

在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变。

不更新主键两步处理：

- 将旧记录进行delete mark 操作
- 根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（改变了主键值，需重新定位插入的位置）。

之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC。

针对UPDATE 语句更新记录主键值的这种情况，在对该记录进行delete mark 操作前，会记录一条类型为TRX_UNDO_DEL_MARK_REC 的undo日志；之后插入新记录时，会记录一条类型为TRX_UNDO_INSERT_REC 的undo日志，也就是说**每对一条记录的主键值做改动时，会记录2条undo日志**。

------

### 通用链表

在写入undo日志的过程中会使用到多个链表，很多链表都有同样的节点结构。

#### List Node结构（链表节点）

![image-20211101094900289](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/80.png)

如图，在表空间内，可以通过**一个页的页号和在页内的偏移量**来唯一定位一个节点的位置，这两个信息也就相当于指向这个节点的一个指针。

#### List Base Node结构（基节点）

![image-20211101095144409](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/81.png)

List Length 表明该链表一共有多少节点。

#### List Base Node 和List Node 组成的链表

![image-20211101095327771](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/82.png)

------

### FIL_PAGE_UNDO_LOG页面

前面说过，表空间其实是由许许多多的页面构成的，页面默认大小为16KB 。用来存储undo日志类型的页面为FIL_PAGE_UNDO_LOG ，如下：

![image-20211101095740317](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/83.png)

File Header与File Trailer是页面通用结构，这里看一下Undo Page Header：

![image-20211101095958545](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/84.png)

- TRX_UNDO_PAGE_TYPE ：本页面准备存储什么种类的undo日志。

可能为前文所说的，TRX_UNDO_INSERT类或TRX_UNDO_PAGE_TYPE类中的日志。

类型为TRX_UNDO_INSERT_REC的undo日志在事务提交后可以直接删除掉，而其他类型的undo日志还需要为的MVCC服务，不能直接删除掉。

- TRX_UNDO_PAGE_START ：表示第一条undo日志在本页面中的起始偏移量。
- TRX_UNDO_PAGE_FREE ：表示最后一条undo日志在本页中结束时的偏移量。
- TRX_UNDO_PAGE_NODE ：代表一个List Node 结构。

------

### Undo页面链表

#### 单个事务中的Undo页面链表

一个事务可能包含多个语句，一个语句可能会对若干条记录进行改动，而对每条记录进行改动前，都需要记录1~2条undo日志，所以一个事务执行过程中可能产生很多undo日志。这些日志可能在一个页面放不下，需要放到多个页面中。这些页面就通过上边介绍的TRX_UNDO_PAGE_NODE 属性连成了链表：

![image-20211101101735477](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/85.png)

需要注意的是，在first undo page 中除了记录Undo Page Header 之外，还会记录其他的一些管理信息。

在一个事务执行过程中，可能混着执行INSERT 、DELETE 、UPDATE 语句，也就意味着会产生不同类型的undo日志。前面说过，同一个Undo页面要么只存储TRX_UNDO_INSERT 大类的undo日志，要么只存储TRX_UNDO_UPDATE 大类的undo日志，所以**在一个事务执行过程中就可能需要2个Undo页面的链表，一个insert undo链表，另一个update undo链表**。

![image-20211101102056246](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/86.png)

又因为，对普通表和临时表的记录改动时产生的undo日志要分别记录，所以在一个事务中最多有4个以Undo页面为节点组成的链表：

![image-21101102056246](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/87.png)



刚刚开启事务时，一个Undo页面链表也不分配，然后执行到相应的Inseret或update语句才进行分配。总之就是，**按需分配，啥时候需要啥时候再分配，不需要就不分配**。

#### 多个事务中的Undo页面链表

为了尽可能提高undo日志的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。

![image-20211101102650315](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/88.png)

------

### undo日志具体写入过程

![image-20211101110846711](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/89.png)

#### Undo Log Segment Header

##### 段的概念

前文说过，段是一个逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。比如一个B+ 树索引被划分成两个段，一个叶子节点段，一个非叶子节点段，这样叶子节点就可以被尽可能的存到一起，非叶子节点被尽可能的存到一起。**每一个段对应一个INODE Entry 结构**，这个INODE Entry 结构描述了这个段的各种信息，比如段的ID ，段内的各种链表基节点，零散页面的页号有哪些等信息。**Segment Header的结构**用来定位一个INODE Entry：

![image-20211101103008846](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/90.png)

整个Segment Header 占用10个字节大小，各个属性的意思如下：

- Space ID of the INODE Entry ： INODE Entry 结构所在的表空间ID。
- Page Number of the INODE Entry ： INODE Entry 结构所在的页面页号。
- Byte Offset of the INODE Ent ： INODE Entry 结构在该页面中的偏移量

##### Undo Log Segment Header

**每一个Undo页面链表都对应着一个段，称之为Undo Log Segment** 。链表中的页面都是从这个段里边申请的，所以它们在Undo页面链表的第一个页面，在**first undo page**中有一个**Undo Log Segment Header**的部分，这个部分中包含了该链表对应的段的segment header 信息以及其他的一些关于这个段的信息。Undo 页面链表的第一个页面：

![image-20211101103623620](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/91.png)

这个Undo 链表的第一个页面比普通页面多了个Undo Log Segment Header：

![image-20211101104924160](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/92.png)

- TRX_UNDO_STATE ：本Undo页面链表处在什么状态。包括：
  - TRX_UNDO_ACTIVE ：活跃状态，也就是一个活跃的事务正在往这个段里边写入undo日志；
  - TRX_UNDO_CACHED ：被缓存的状态。处在该状态的Undo页面链表等待着之后被其他事务重用。TRX_UNDO_TO_FREE ：对于insert undo 链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态；
  - TRX_UNDO_TO_PURGE ：对于update undo 链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态；
  - TRX_UNDO_PREPARED ：包含处于PREPARE阶段的事务产生的undo日志，事务的PREPARE阶段是在所谓的分布式事务中才出现的。
- TRX_UNDO_LAST_LOG ：本Undo页面链表中最后一个Undo Log Header的位置。
- TRX_UNDO_FSEG_HEADER ：本Undo页面链表对应的段的Segment Header信息。
- TRX_UNDO_PAGE_LIST ： Undo页面链表的基节点。

上边说Undo页面的Undo Page Header部分有一个12字节大小的**TRX_UNDO_PAGE_NODE** 属性，这个属性代表一个List Node 结构。每一个Undo页面都包含Undo Page Header 结构，这些页面就可以通过这个属性连成一个链表。这个TRX_UNDO_PAGE_LIST 属性代表着这个链表的基节点，当然这个基节点只存在于Undo页面链表的第一个页面，也就是first undo page 中。

#### Undo Log Header

同一个事务向一个Undo页面链表中写入的undo日志算是一个组，即一条链表算一个组。在每写入一组undo日志时，都会在这组undo日志前先记录一下关于这个组的一些属性，把存储这些属性的地方称之为**Undo Log Header** 。所以Undo页面链表的第一个页面在真正写入undo日志前，其实都会被填充**Undo Page Header** 、**Undo Log SegmentHeader 、Undo Log Header**这三个部分：

![image-20211101110331036](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/93.png)

Undo Log Header 具体的结构如下：

![image-20211101110530521](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/94.png)

- TRX_UNDO_TRX_ID ：生成本组undo日志的事务id 。
- TRX_UNDO_TRX_NO ：事务提交后生成的一个需要序号，使用此序号来标记事务的提交顺序（先提交的此序号小，后提交的此序号大）。
- TRX_UNDO_DEL_MARKS ：标记本组undo 日志中是否包含由于Delete mark 操作产生的undo日志。
- TRX_UNDO_LOG_START ：表示本组undo 日志中第一条undo日志的在页面中的偏移量。
- TRX_UNDO_XID_EXISTS ：本组undo日志是否包含XID信息。
- TRX_UNDO_DICT_TRANS ：标记本组undo日志是不是由DDL语句产生的。
- TRX_UNDO_TABLE_ID ：如果TRX_UNDO_DICT_TRANS 为真，本属性就表示DDL语句操作的表的table id 。
- TRX_UNDO_NEXT_LOG ：下一组的undo日志在页面中开始的偏移量。
- TRX_UNDO_PREV_LOG ：上一组的undo日志在页面中开始的偏移量。
- TRX_UNDO_HISTORY_NODE ：一个12字节的List Node 结构，代表一个称之为History 链表的节点。

#### 写入过程小结

对于没有被重用的Undo页面链表来说，链表的第一个页面，也就是first undo page 在真正写入undo日志前，会填充Undo Page Header 、Undo Log Segment Header 、Undo Log Header 这3个部分，之后才开始正式写入undo日志。

对于其他的页面来说，也就是normal undo page在真正写入undo日志前，只会填充Undo Page Header 。链表的List Base Node存放到first undo page的Undo Log Segment Header部分，List Node信息存放到每一个Undo页面的undo Page Header部分。

所以画一个Undo页面链表的示意图就是这样：

![image-20211101110846711](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/95.png)

------

### 重用Undo页面

#### Undo页面链表可以重用的两个条件

- 该链表中只包含一个Undo页面。

- 该Undo页面已经使用的空间小于整个页面空间的3/4。

#### 两种链表重用策略

Undo页面链表按照存储的undo日志所属的大类可以被分为insert undo链表和update undo链表两种，这两种链表在被重用时的策略也是不同的：

- insert undo链表

所以在某个事务提交后，重用这个事务的insert undo链表（这个链表中只有一个页面）时，可以直接把之前事务写入的一组undo日志覆盖掉，从头开始写入新事务的一组undo日志。这是因为，insert undo链表中存储类型为TRX_UNDO_INSERT_REC 的undo日志，这种类型的undo日志在事务提交之后就没用了，可以直接被清除掉。

![image-20211101112835815](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/96.png)

- update undo链表

在一个事务提交后，它的update undo链表中的undo日志也不能立即删除掉（这些日志用于MVCC）。所以如果之后的事务想重用update undo链表时，就不能覆盖之前事务写入的undo日志。这样就相当于在同一个Undo页面中写入了多组的undo日志。

![image-20211101113043375](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/97.png)

------

### 回滚段

#### 回滚段的概念

一个事务在执行过程中最多可以分配4个Undo页面链表，为了更好地管理这些链表，设计了一个**Rollback Segment Header**的页面，在**这个页面中存放了各个Undo页面链表的frist undo page 的页号，把这些页号称之为undo slot，一个undo slot代表一个Undo页面链表的第一个页面的页号**。

可以这样理解，每个Undo页面链表都相当于是一个班，这个链表的first undo page就相当于这个班的班长，找到了这个班的班长，就可以找到班里的其他同学（其他同学相当于normal undo page ）。有时候学校需要向这些班级传达一下任务，就需要把班长都召集在会议室，这个Rollback Segment Header 就相当于是一个会议室。

Rollback Segment Header 的页面：

![image-20211101113529101](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/98.png)

**每一个Rollback Segment Header页面都对应着一个段，这个段就称为回滚段（Rollback Segment ）**。与其他的段段不同，Rollback Segment里其实**只有一个页面**。

- TRX_RSEG_MAX_SIZE ：本Rollback Segment 中管理的所有Undo页面链表中的Undo页面数量之和的最大值。
- TRX_RSEG_HISTORY_SIZE ： History 链表占用的页面数量。
- TRX_RSEG_HISTORY ： History 链表的基节点。
- TRX_RSEG_FSEG_HEADER ：本Rollback Segment 对应的10字节大小的Segment Header 结构，通过它可以找到本段对应的INODE Entry 。
- TRX_RSEG_UNDO_SLOTS ：各个Undo页面链表的first undo page 的页号集合，也就是**undo slot 集合**。

#### 多个回滚段

一个事务执行过程中最多分配4 个Undo页面链表，而一个回滚段里只有1024 个undo slot。一共可以定义128 个回滚段，每个回滚段都对应着一个Rollback Segment Header 页面，所以一共有128个Rollback Segment Header 页面。这些页面存储在**系统表空间的第5 号页面**中：

![image-20211101133414144](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/99.png)

每个8字节的格子的构造：

![image-20211101133441669](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/100.png)

每个8字节的格子其实由两部分组成：

- 4字节大小的Space ID ，代表一个表空间的ID。
- 4字节大小的Page number ，代表一个页号。

也就是说每个8字节大小的格子相当于一个指针，指向某个表空间中的某个页面，这些页面就是RollbackSegment Header 。但这里没有表空间id，这也就意味着不同的回滚段可能分布在不同的表空间中

总的来说：

在系统表空间的第5 号页面中存储了128个Rollback Segment Header页面地址，每个Rollback Segment Header 就相当于一个回滚段。在Rollback Segment Header 页面中，又包含1024 个undo slot ，每个undo slot 都对应一个Undo页面链表。

![image-233441669](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/101.png)

#### 回滚段的分类

把这128个回滚段给编一下号，最开始的回滚段称之为第0号回滚段，之后依次递增，最后一个回滚段就称之为第127号回滚段。

- 第0 号、第33～127 号回滚段属于一类。

其中第0 号回滚段必须在系统表空间中（就是说第0 号回滚段对应的Rollback Segment Header 页面必须在系统表空间中），第33～127 号回滚段既可以在系统表空间中，也可以在自己配置的undo 表空间中。

- 第1～32 号回滚段属于一类。

这些回滚段必须在临时表空间（对应着数据目录中的ibtmp1 文件）中。

如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段，再分别到这两个回滚段中分配对应的undo slot 。

总结一下针对普通表和临时表划分不同种类的回滚段的原因：**在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志**。

#### roll_pointer的组成

有些类型的日志包含roll_pointer的属性，本质上是一个指向一条undo日志地址的指针。

![image-20211101135105697](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/102.png)

- is_insert：表示该指针指向的undo日志是否是TRX_UNDO_INSERT大类的undo日志。
- rseg id：表示该指针指向的undo日志的编号。
- page number：表示该指针指向的undo日志所在页面的页号。
- offset：表示该指针指向的undo日志在页面中的偏移量。

#### 为事务分配Undo页面链表的详细过程

梳理一下事务执行过程中分配Undo页面链表时的完整过程：

- 事务在执行过程中对普通表的记录首次做改动之前，**首先系统表空间的第5号页面中分配一个回滚段**（其实就是获取一个Rollback Segment Header 页面的地址）。一旦某个回滚段被分配给了这个事务，那么之后该事务中再对普通表的记录做改动时，就不会重复分配。
- 在分配到回滚段后，首先，**针对不同的操作，看一下这个回滚段的两个cached链表有没有已经缓存了的undo slot** 。如果有缓存的undo slot，那么就把这个缓存的undo slot 分配给该事务。
- 如果没有缓存的undo slot 可供分配，那么就要**到Rollback Segment Header 页面中找一个可用的undo slot 分配给当前事务**。
- 找到可用的undo slot 后，如果该undo slot 是从cached链表中获取的，那么它对应的Undo Log Segment 已经分配了，否则的话需要重新分配一个Undo Log Segment ，然后**从该Undo Log Segment 中申请一个页面作为Undo页面链表的first undo page** 。
- 然后事务就可以把undo日志写入到上边申请的Undo页面链表里。

------

参考：

[详细分析MySQL事务日志(redo log和undo log) - 骏马金龙 - 博客园 (cnblogs.com)](https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html#auto_id_11)

[【图文详解】MySQL系列之redo log、undo log和binlog详解 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1801920)

------

------

## bin log（二进制日志）

### 基本介绍

binlog：binlog是Mysql sever层维护的一种二进制日志，与innodb引擎中的redo/undo log是完全不同的日志；其主要是用来记录对mysql数据更新或潜在发生更新的SQL语句，并以"事务"的形式保存在磁盘中。默认情况下没有开启，需要到MySQL的配置文件中开启，并配置MySQL日志的格式。

作用：

- 查看数据库的变更历史
- 数据库增量备份和恢复：通过mysqlbinlog工具恢复数据
- Mysql的复制（主从数据库的复制）：MySQL Replication在Master端开启binlog，Master把它的二进制日志传递给slaves并回放来达到master-slave数据一致的目的

------

### binlog管理

- 配置binlog日志存放位置，配置文件位置 : /usr/my.cnf

```ini
# "mysql-bin：存放binlog路径目录"
[mysqld]
log-bin=mysql-bin 
```

binlog信息查询binlog开启后，可以在配置文件中查看其位置信息，也可以在myslq命令行中查看。

- 查看binlog位置信息

```sql
show variables like '%log_bin%';
```

> ```text
> +---------------------------------+-------------------------------------+
> | Variable_name                   | Value                               |
> +---------------------------------+-------------------------------------+
> | log_bin                         | ON                                  |
> | log_bin_basename                | /var/lib/mysql/3306/mysql-bin       |
> | log_bin_index                   | /var/lib/mysql/3306/mysql-bin.index |
> | log_bin_trust_function_creators | OFF                                 |
> +---------------------------------+-------------------------------------+
> ```

binlog文件开启binlog后，会在数据目录（默认）产生`host-bin.n`（具体binlog信息）文件及`host-bin.index`索引文件（记录binlog文件列表）。

当binlog日志写满（binlog大小max_binlog_size，默认1G），或者数据库重启才会生产新文件，但是也可通过手工进行切换让其重新生成新的文件（flush logs）；另外，如果正使用大的事务，由于一个事务不能横跨两个文件，因此也可能在binlog文件未满的情况下刷新文件。

- 查看binlog文件列表

```sql
show binary logs; 
```

> ```text
> +------------------+-----------+
> | Log_name         | File_size |
> +------------------+-----------+
> | mysql-bin.000001 |       177 |
> | mysql-bin.000002 |       177 |
> +------------------+-----------+
> ```

- 查看binlog的状态

```sql
show master status;
```

```text
 +------------------+----------+--------------+------------------+-------------------+
 | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
 +------------------+----------+--------------+------------------+-------------------+
 | mysql-bin.000007 |      120 |              |                  |                   |
 +------------------+----------+--------------+------------------+-------------------+
```

`show master status`：可查看当前二进制日志文件的状态信息，显示正在写入的二进制文件，及当前position。

- flush刷新binlog日志

```sql
-- 自此刻开始产生一个新编号的binlog日志文件
flush logs;
```

每当mysqld服务重启时，会自动执行此命令，刷新bin log日志；在mysqld dump备份数据时加 -F选项 也会刷新binlog日志

- 清空（重置）binlog日志文件

```sql
reset master;
```

------

### binlog内容

默认情况下binlog日志是二进制格式，无法直接查看。可使用两种方式进行查看：

- 通过mysqlbinlog查看

```sql
mysqlbinlog: /usr/bin/mysqlbinlog  mysql-bin.000007
```

mysqlbinlog是mysql官方提供的一个binlog查看工具

也可使用–read-from-remote-server从远程服务器读取二进制日志

还可使用--start-position --stop-position、--start-time、 --stop-time精确解析binlog日志

> --start-datetime：从二进制日志中读取指定等于时间戳或者晚于本地计算机的时间
>
> --stop-datetime：从二进制日志中读取指定小于时间戳或者等于本地计算机的时间 取值和上述一样
>
> --start-position：从二进制日志中读取指定position 事件位置作为开始。
>
> --stop-position：从二进制日志中读取指定position 事件位置作为事件截至

> 举例：
>
> ```text
> 截取位置1190-1352 binlog如下：
>         ***************************************************************************************
>         # at 1190   //事件的起点
>         #171223 21:56:26 server id 123  end_log_pos 1190 CRC32 0xf75c94a7 	Intvar
>         SET INSERT_ID=2/*!*/;
>         #171223 21:56:26 server id 123  end_log_pos 1352 CRC32 0xefa42fea 	Query	thread_id=4	exec_time=0	error_code=0
>         SET TIMESTAMP=1514123786/*!*/;              //开始事务的时间起点 (每个at即为一个event)
>         insert into tb_person  set name="name__2", address="beijing", sex="man", other="nothing"  //sql语句
>         /*!*/;
>         # at 1352
>         #171223 21:56:26 server id 123  end_log_pos 1383 CRC32 0x72c565d3 	Xid = 5 //执行时间，及位置戳，Xid:事件指示提交的XA事务
>         ***************************************************************************************
> ```

- 命令行解析

```sql
SHOW BINLOG EVENTS
    [IN 'log_name'] -- 要查询的binlog文件名
    [FROM pos]  
    [LIMIT [offset,] row_count]  
```

> 举例：
>
> ```sql
> show binlog events in 'mysql-bin.000007' from 1190 limit 2\G
> ```
>
> ```text
> 1190-135如下：
>         *************************** 13. row ***************************
>            Log_name: mysql-bin.000007
>                 Pos: 1190
>          Event_type: Query  //事件类型
>           Server_id: 123
>         End_log_pos: 1352   //结束pose点，下个事件的起点
>                Info: use `test`; insert into tb_person  set name="name__2", address="beijing", sex="man", other="nothing"
>         *************************** 14. row ***************************
>            Log_name: mysql-bin.000007
>                 Pos: 1352
>          Event_type: Xid
>           Server_id: 123
>         End_log_pos: 1383
>                Info: COMMIT /* xid=51 */
> ```

------

### binlog格式

#### 格式配置语法

Mysql binlog日志有ROW，Statement，MiXED三种格式。

配置二进制日志格式的两种方式：my.cnf配置和命令行配置。

通过my.cnf配置

```ini
[mysqld]
binlog_format=STATEMENT
```

命令行配置

```sql
-- 查看binlog_format
show variables like 'binlog_format';

-- 修改binlog_format
set globle binlog_format='STATEMENT/ROW/MIXED';
```

#### STATEMENT

该日志格式在日志文件中记录的都是SQL语句，每一条对数据进行修改的SQL都会记录在日志文件中，通过Mysql提供的mysqlbinlog工具，可以清晰的查看到每条语句的文本。主从复制的时候，从库（slave）会将日志解析为原文本，并在从库重新执行一次。

优点：

只需要记录执行语句的细节和上下文环境，避免了记录每一行的变化，在一些修改记录较多的情况下相比ROW level能大大减少binlog日志量，节约IO，提高性能；还可以用于实时的还原；同时主从版本可以不一样，从服务器版本可以比主服务器版本高。

缺点：

为了保证sql语句能在slave上正确执行，必须记录上下文信息，以保证所有语句能在slave得到和在master端执行时候相同的结果；另外，主从复制时，存在部分函数（如sleep）及存储过程在slave上会出现与master结果不一致的情况，而相比Row level记录每一行的变化细节，绝不会发生这种不一致的情况

#### ROW

该日志格式在日志文件中记录的是每一行的数据变更，而不是记录SQL语句。

优点：

能非常清晰的记录下每行数据的修改细节，不需要记录上下文相关信息，因此不会发生某些特定情况下的procedure、function、及trigger的调用触发无法被正确复制的问题，任何情况都可以被复制，且能加快从库重放日志的效率，保证从库数据的一致性

缺点：

由于所有的执行的语句在日志中都将以每行记录的修改细节来记录，因此，可能会产生大量的日志内容，干扰内容也较多；比如一条update语句，如修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中，实际等于重建了表。

> 举例：
>
> 执行SQL语句 ： update tb_book set status='1' , 
>
> 如果是STATEMENT 日志格式，在日志中会记录一行SQL文件； 
>
> 如果是ROW，由于是对全表进行更新，也就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更。

#### MIXED

这是目前MySQL默认的日志格式，即混合了STATEMENT 和 ROW两种格式。默认情况下采用STATEMENT，但是在一些特殊情况下采用ROW来进行记录。MIXED 格式能尽量利用两种模式的优点，而避开它们的缺点。

不过，新版本的MySQL对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更；因此，现在一般使用row level即可。

#### 选取规则

如果是采用 INSERT，UPDATE，DELETE 直接操作表的情况，则日志格式根据 binlog_format 的设定而记录

如果是采用 GRANT，REVOKE，SET PASSWORD 等管理语句来做的话，那么无论如何都采用statement模式记录

#### 常用命令小结

- binlog文件会随服务的启动创建一个新文件
- 通过flush logs可以手动刷新日志，生成一个新的binlog文件
- 通过show master status可以查看binlog的状态
- 通过reset master可以清空binlog日志文件
- 通过mysqlbinlog工具可以查看binlog日志的内容
- 通过执行DML语句，mysql会自动记录bin log

参考

[Binlog详解 - 简书 (jianshu.com)](https://www.jianshu.com/p/ea666baf0d82)

[腾讯工程师带你深入解析 MySQL binlog - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/33504555)

[mysql binlog详解 - Presley - 博客园 (cnblogs.com)](https://www.cnblogs.com/Presley-lpc/p/9619571.html)

------

------

# 主从复制

## 基本介绍

MySQL 主从复制：指指将主数据库的DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。

主从复制中分为主服务器（master）和从服务器（slave），主服务器负责写，而从服务器负责读，Mysql的主从复制的过程是一个异步的过程。

MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制。

MySQL 主从复制主要用途：

- 读写分离：在开发工作中，有时候会遇见某个sql 语句需要锁表，导致暂时不能使用读的服务，这样就会影响现有业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。
- 数据实时备份：当系统中某个节点发生故障时，可以方便的故障切换。
- 高可用HA
- 架构扩展：随着系统中业务访问量的增大，如果是单机部署数据库，就会导致I/O访问频率过高。有了主从复制，增加多个数据存储节点，将负载分布在多个从节点上，降低单机磁盘I/O访问的频率，提高单个机器的I/O性能。

------

## MySQL 主从形式

![image-20211120165725821](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/103.png)

一主一从和一主多从：最常见的主从架构，实施起来简单并且有效，不仅可以实现HA，而且还能读写分离，进而提升集群的并发能力。

多主一从（MySQL5.7开始）：可以将多个mysql数据库备份到一台存储性能比较好的服务器上。

双主复制：也就是互做主从复制，每个master既是master，又是另外一台服务器的slave。这样任何一方所做的变更，都会通过复制应用到另外一方的数据库中。

级联复制：部分slave的数据同步不连接主节点，而是连接从节点。因为如果主节点有太多的从节点，就会损耗一部分性能用于replication，可以让3~5个从节点连接主节点，其它从节点作为二级或者三级与从节点连接，这样不仅可以缓解主节点的压力，并且对数据一致性没有负面影响。

------

## 复制原理

### 主从复制原理图

![img](https://pic1.zhimg.com/v2-1b0c3f31bd398c39b9e0930059b0ca24_r.jpg)

主从复制主要依赖的是 binlog，MySQL 默认是异步复制，需要三个线程：

- **binlog dump thread**：在主库事务提交时，负责把数据变更记录在二进制日志文件 binlog 中，并通知 slave 有数据更新
- **I/O thread**：负责从主服务器上拉取二进制日志，并将 binlog 日志内容依次写到 relay log 中转日志的最末端，并将新的 binlog 文件名和 offset 记录到 master-info 文件中，以便下一次读取日志时从指定 binlog 日志文件及位置开始读取新的 binlog 日志内容
- **SQL thread**：监测本地 relay log 中新增了日志内容，读取中继日志并重做其中的 SQL 语句，从库在 relay-log.info 中记录当前应用中继日志的文件名和位置点以便下一次执行

对于每一个主从连接，都需要三个线程来完成。当主节点有多个从节点时，主节点会为每一个当前连接的从节点建一个binary log dump 线程，而每个从节点都有自己的I/O线程，SQL线程。从节点用两个线程将从主库拉取更新和执行分成独立的任务，这样在执行同步数据任务的时候，不会降低读操作的性能。比如，如果从节点没有运行，此时I/O线程可以很快从主节点获取更新，尽管SQL线程还没有执行。如果在SQL线程执行之前从节点服务停止，至少I/O线程已经从主节点拉取到了最新的变更并且保存在本地relay日志中，当服务再次起来之后，就可以完成数据的同步。

### 复制过程

- Master 主库在事务提交时，会把数据变更作为时间 Events 记录在二进制日志文件 Binlog 中。
- 主库推送二进制日志文件 Binlog 中的日志事件到从库的中继日志 Relay Log 。
- slave重做中继日志中的事件，将改变反映它自己的数据。

------

## 主从复制模式

MySQL 主从复制默认是异步的模式。MySQL增删改操作会全部记录在binary log中，当slave节点连接master时，会主动从master处获取最新的bin log文件。并把bin log中的sql relay。

### 异步模式（mysql async-mode）

异步模式如下图所示，这种模式下，主节点不会主动push bin log到从节点，这样有可能导致failover的情况下，也许从节点没有即时地将最新的bin log同步到本地。

![img](https://pic1.zhimg.com/v2-c15bfffe3e398eafc7e0ffdaeebfcaac_r.jpg)

### 半同步模式(mysql semi-sync)

这种模式下主节点只需要接收到其中一台从节点的返回信息，就会commit；否则需要等待直到超时时间然后切换成异步模式再提交；这样做的目的可以使主从数据库的数据延迟缩小，可以提高数据安全性，确保了事务提交后，binlog至少传输到了一个从节点上，不能保证从节点将此事务更新到db中。性能上会有一定的降低，响应时间会变长。如下图所示：

![img](https://pic2.zhimg.com/v2-d9ac9c5493d1d772f5bf57ede089f0d5_r.jpg)

半同步模式不是mysql内置的，从mysql 5.5开始集成，需要master 和slave 安装插件开启半同步模式。

### 全同步模式

全同步模式是指主节点和从节点全部执行了commit并确认才会向客户端返回成功。

#### binlog记录格式

二进制日志中说过，MySQL 主从复制有三种方式：基于SQL语句的复制（statement-based replication，SBR），基于行的复制（row-based replication，RBR)，混合模式复制（mixed-based replication,MBR)。对应的binlog文件的格式也有三种：STATEMENT,ROW,MIXED。

**基于binlog复制实现的工作原理**：

- Master将数据改变记录到二进制日志(binary log)中
- Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容
- Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置
- Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master从某个bin-log的哪个位置开始往后的日志内容
- Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行

#### GTID复制模式

在传统的复制里面，当发生故障，需要主从切换，需要找到binlog和pos点，然后将主节点指向新的主节点，相对来说比较麻烦，也容易出错。在MySQL 5.6里面，不用再找binlog和pos点，只需要知道主节点的ip，端口，以及账号密码就行，因为复制是自动的，MySQL会通过内部机制GTID自动找点同步。

多线程复制（基于库），在MySQL 5.6以前的版本，slave的复制是单线程的。一个事件一个事件的读取应用。而master是并发写入的，所以延时是避免不了的。唯一有效的方法是把多个库放在多台slave，这样又有点浪费服务器。在MySQL 5.6里面，可以把多个表放在多个库，这样就可以使用多线程复制。

**基于GTID复制实现的工作原理**：

- 主节点更新数据时，会在事务前产生GTID，一起记录到binlog日志中。
- 从节点的I/O线程将变更的bin log，写入到本地的relay log中。
- SQL线程从relay log中获取GTID，然后对比本地binlog是否有记录（所以MySQL从节点必须要开启binary log）。
- 如果有记录，说明该GTID的事务已经执行，从节点会忽略。
- 如果没有记录，从节点就会从relay log中执行该GTID的事务，并记录到bin log。
- 在解析过程中会判断是否有主键，如果没有就用二级索引，如果有就用全部扫描。

#### 基于binlog的主从复制示例

> 举例：基于binlog的主从复制
>
> ```text
>    a.配置master
>         主要包括设置复制账号，并授予REPLICATION SLAVE权限，具体信息会存储在于master.info文件中，及开启binlog；
>         mysql> CREATE USER 'test'@'%' IDENTIFIED BY '123456';
>         mysql> GRANT REPLICATION SLAVE ON *.* TO 'test'@'%';
>         mysql> show variables like "log_bin";
>             +---------------+-------+
>             | Variable_name | Value |
>             +---------------+-------+
>             | log_bin       | ON    |
>             +---------------+-------+
>         查看master当前binlogmysql状态：mysql> show master status;
>             +------------------+----------+--------------+------------------+-------------------+
>             | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
>             +------------------+----------+--------------+------------------+-------------------+
>             | mysql-bin.000003 |      120 |              |                  |                   |
>             +------------------+----------+--------------+------------------+-------------------+
>         建表插入数据：
>             CREATE TABLE `tb_person` (
>         	   `id` int(11) NOT NULL AUTO_INCREMENT,
>                `name` varchar(36) NOT NULL,                           
>                `address` varchar(36) NOT NULL DEFAULT '',    
>                `sex` varchar(12) NOT NULL DEFAULT 'Man' ,
>         	   `other` varchar(256) NOT NULL ,
>                PRIMARY KEY (`id`)
>              ) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8;
>              
>         	 insert into tb_person  set name="name1", address="beijing", sex="man", other="nothing";
>         	 insert into tb_person  set name="name2", address="beijing", sex="man", other="nothing";
>         	 insert into tb_person  set name="name3", address="beijing", sex="man", other="nothing";
>         	 insert into tb_person  set name="name4", address="beijing", sex="man", other="nothing";
>     b.配置slave
>         Slave的配置类似master，需额外设置relay_log参数，slave没有必要开启二进制日志，如果slave为其它slave的master，须设置bin_log
>     c.连接master
>         mysql> CHANGE MASTER TO
>            MASTER_HOST='10.108.111.14',
>            MASTER_USER='test',
>            MASTER_PASSWORD='123456',
>            MASTER_LOG_FILE='mysql-bin.000003',
>            MASTER_LOG_POS=120;
>     d.show slave status;
>         mysql> show slave status\G
>         *************************** 1. row ***************************
>                        Slave_IO_State:   ---------------------------- slave io状态，表示还未启动
>                           Master_Host: 10.108.111.14  
>                           Master_User: test  
>                           Master_Port: 20126  
>                         Connect_Retry: 60   ------------------------- master宕机或连接丢失从服务器线程重新尝试连接主服务器之前睡眠时间
>                       Master_Log_File: mysql-bin.000003  ------------ 当前读取master binlog文件
>                   Read_Master_Log_Pos: 120  ------------------------- slave读取master binlog文件位置
>                        Relay_Log_File: relay-bin.000001  ------------ 回放binlog
>                         Relay_Log_Pos: 4   -------------------------- 回放relay log位置
>                 Relay_Master_Log_File: mysql-bin.000003  ------------ 回放log对应maser binlog文件
>                      Slave_IO_Running: No
>                     Slave_SQL_Running: No
>                   Exec_Master_Log_Pos: 0  --------------------------- 相对于master从库的sql线程执行到的位置
>                 Seconds_Behind_Master: NULL
>         Slave_IO_State, Slave_IO_Running, 和Slave_SQL_Running为NO说明slave还没有开始复制过程。
>     e.启动复制
>         start slave
>     f.再次观察slave状态
>         mysql> show slave status\G
>         *************************** 1. row ***************************
>                        Slave_IO_State: Waiting for master to send event -- 等待master新的event
>                           Master_Host: 10.108.111.14
>                           Master_User: test
>                           Master_Port: 20126
>                         Connect_Retry: 60
>                       Master_Log_File: mysql-bin.000003
>                   Read_Master_Log_Pos: 3469  ---------------------------- 3469  等于Exec_Master_Log_Pos，已完成回放
>                        Relay_Log_File: relay-bin.000002                    ||
>                         Relay_Log_Pos: 1423                                ||
>                 Relay_Master_Log_File: mysql-bin.000003                    ||
>                      Slave_IO_Running: Yes                                 ||
>                     Slave_SQL_Running: Yes                                 ||
>                   Exec_Master_Log_Pos: 3469  -----------------------------3469  等于slave读取master binlog位置，已完成回放
>                 Seconds_Behind_Master: 0
>         可看到slave的I/O和SQL线程都已经开始运行，而且Seconds_Behind_Master=0。Relay_Log_Pos增加，意味着一些事件被获取并执行了。
>         
>         最后看下如何正确判断SLAVE的延迟情况，判定slave是否追上master的binlog：
>         1、首先看 Relay_Master_Log_File 和 Maser_Log_File 是否有差异；
>         2、如果Relay_Master_Log_File 和 Master_Log_File 是一样的话，再来看Exec_Master_Log_Pos 和 Read_Master_Log_Pos 的差异，对比SQL线程比IO线程慢了多少个binlog事件；
>         3、如果Relay_Master_Log_File 和 Master_Log_File 不一样，那说明延迟可能较大，需要从MASTER上取得binlog status，判断当前的binlog和MASTER上的差距；
>         4、如果以上都不能发现问题，可使用pt_heartbeat工具来监控主备复制的延迟。
>         
>     g.查询slave数据，主从一致
>         mysql> select * from tb_person;
>             +----+-------+---------+-----+---------+
>             | id | name  | address | sex | other   |
>             +----+-------+---------+-----+---------+
>             |  5 | name4 | beijing | man | nothing |
>             |  6 | name2 | beijing | man | nothing |
>             |  7 | name1 | beijing | man | nothing |
>             |  8 | name3 | beijing | man | nothing |
>             +----+-------+---------+-----+---------+
> ```

参考：

[MySQL 的主从复制实践-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/article/744929)

[腾讯工程师带你深入解析 MySQL binlog - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/33504555)

------

------

# 事务

## 事务简介

事务（Transaction）是访问和更新数据库的程序执行单元；事务就是完成同⼀业务的一个或多个DML操作，这些语句要么都执行，要么都不执行。在 MySQL 中，事务支持是在引擎层实现的，只有InnoDB引擎支持事务。

------

## 事务操作

管理事务的三个步骤

- 开启事务：记录回滚点，并通知服务器，将要执行一组操作，要么同时成功、要么同时失败

```sql
-- 显式开启
BEGIN;
START TRANSACTION;
-- 隐式开启
set autocommit=0; -- 将线程的自动提交关掉, 执行一个sql语句就开启事务
set autocommit=1; -- 将线程的自动提交打开, 每个SQL语句都会被当做一个事务执行提交操作
```

- 执行 SQL 语句：执行具体的一条或多条 SQL 语句

- 结束事务（回滚 | 提交）

  - 回滚：出现问题，数据恢复到开启事务时的状态

  ```sql
  -- 回滚事务
  ROLLBACK;
  ```

  - 提交：没有问题，数据进行更新

  ```sql
  -- 提交事务
  COMMIT;
  ```

------

## 事物ACID特性

- **Atomicity（原子性）**：一个事务（transaction）中的所有操作，要么全部完成，要么全部失败，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。

- **Consistency（一致性）**：在事务开始之前和事务结束以后，数据库的完整性没有被破坏，事务执行的前后都是合法的数据状态。这表示写入的数据必须完全满足：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）等。 

- **Isolation（隔离性）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读、提交读、可重复读和串行化。 

- **Durability（持久性）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

------

## 事务隔离级别

数据库允许多个事务并⾏，多个事务之间是隔离的、相互独⽴的；如果事务之间不相互隔离并且操作**同⼀数据**时，可能会导致数据的⼀致性被破坏。

### 并发事务产生的问题

- 脏写：一个事务修改了另一个未提交事务修改过的数据

- 脏读：⼀个事务读取到了另⼀个未提交事务中的数据 

- 不可重复读（虚读）：在同⼀个事务中，两次查询操作读取到记录不⼀致 

- 幻读：在同⼀个事务中，两次查询操作读取的记录数量不一致。

### 事务隔离级别

- 读未提交（Read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。Oracle 数据库的默认隔离级别。
- 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。即，事务A执⾏第⼀次查询之后，在事务A结束之前其他事务不能修改对应的数据。但可以插入其他数据。
- 串行化（Serializable）：同一时间内，只允许⼀个事务对数据表进⾏操作。对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

不同隔离级别下，可能产生的并发问题：

|     隔离级别     | 脏读 | 不可重复读 | 幻读 |
| :--------------: | :--: | :--------: | :--: |
| READ-UNCOMMITTED |  √   |     √      |  √   |
|  READ-COMMITTED  |  ×   |     √      |  √   |
| REPEATABLE-READ  |  ×   |     ×      |  √   |
|   SERIALIZABLE   |  ×   |     ×      |  ×   |

------

## 设置事务隔离级别

MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。

- 查看事物的隔离级别

```sql
SHOW VARIABLES LIKE 'transaction_isolation';

-- MySQL5.7后用 transaction_isolation 替换 tx_isolation的
SELECT @@tx_isolation;
```

- 设置事务的隔离级别

```sql
-- 运行中修改
-- level可选：REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED | SERIALIZABLE
-- 使用了 GLOBAL 关键字（对全局范围产生影响）：
	-- 只对执行完该语句之后产生的会话起作用；
	-- 当前已经存在的会话无效。
-- 使用 SESSION 关键字（在会话范围影响）：
	-- 对当前会话的所有后续的事务有效；
	-- 该语句可以在已经开启的事务中间执行，但不会影响当前正在执行的事务；
	-- 如果在事务之间执行，则对后续的事务有效。
-- 两个关键字都不用（只对执行语句后的下一个事务产生影响）：
	-- 只对当前会话中下一个即将开启的事务有效，下一个事务执行完后，后续事务将恢复到之前的隔离级别；
	-- 该语句不能在已经开启的事务中间执行，会报错的。
SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level;

-- 启动前修改
-- 作用范围 全局
SET GLOBAL transaction_isolation = 某个隔离级别;
SET @@GLOBAL.var_name = 某个隔离级别;
-- 作用范围 会话
SET SESSION var_name = 某个隔离级别;
SET @@SESSION.var_name = 某个隔离级别;
SET var_name = 某个隔离级别;
-- 下一个隔离级别
SET @@var_name = 某个隔离级别;
```

------

------

# MVCC

## MVCC简述

MVCC 全称 Multi-Version Concurrency Control，即多版本并发控制，用来解决读写冲突的无锁并发控制，提高数据库的并发性能。

同一行数据平时发生读写请求时，会上锁阻塞住。但mvcc在RC或RR隔离级别下处理读—写请求时，做到不用加锁。

这个读是指的快照读，而不是当前读，当前读是一种加锁操作，是悲观锁。

### 快照读与当前读

- 当前读：读取的数据库记录，都是当前最新的版本，会对当前读取的数据进行加锁，防止其他事务修改数据。是悲观锁的一种操作。包括：

```sql
select lock in share mode (共享锁)

select for update (排他锁)

update (排他锁)

insert (排他锁)

delete (排他锁)

串行化事务隔离级别
```

- 快照读：快照读的实现是基于多版本并发控制，即MVCC，既然是多版本，那么快照读读到的数据不一定是当前最新的数据，有可能是之前历史版本的数据。包括：

```sql
不加锁的select操作（注：事务级别RC与RR）
```

快照读与mvcc的关系：MVCCC是“维持一个数据的多个版本，使读写操作没有冲突”的一个抽象概念。这个概念需要具体功能去实现，这个具体实现就是快照读。

### 数据库并发场景

- 读-读：不存在任何问题，也不需要并发控制

- 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读

- 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失

### MVCC解决并发哪些问题？

mvcc用来解决读—写冲突的无锁并发控制，就是**为事务分配单向增长的时间戳**。为每个数据修改保存一个版本，版本与事务时间戳相关联。

读操作只读取该事务开始前的数据库快照。

#### 解决问题如下

- 并发读-写时：可以做到读操作不阻塞写操作，同时写操作也不会阻塞读操作。

- 解决脏读、幻读、不可重复读等事务隔离问题，但不能解决上面的写-写 更新丢失问题。

#### 提高并发性能的组合拳

- MVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突

- MVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决写写冲突

------

## MVCC实现原理

MVCC主要是由**版本链**，**undo日志** ，**Read View** 来实现的

### 版本链

对于使用InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列 **trx_id** 、 **roll_pointer**和**row_id** 。

- trx_id (事务id)：6字节，最近修改该记录的事务ID，记录创建这条记录或最后一次修改该记录的事务ID。
- roll_pointer (回滚指针)：7字节，指向这条记录的上一个版本（存储于rollback segment里）。每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到undo日志中，这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。
- row_id (自增行id)：隐含的自增ID（隐藏主键），当创建的表中有主键或者非NULL的UNIQUE键时，不会创建row_id 列。
- 实际还有一个delete_flag隐藏字段，记录被更新或删除并不代表真的删除，而是delete_flag变了（数据页部分说过）。

假设插入一条记录的事务id 为80 ，那么此刻该条记录的示意图如下所示：

![image-20211102164824209](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/104.png)

每次对记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer 属性（ INSERT 操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表。

![image-20211102165401332](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/105.png)

对该记录每次更新后，都会将旧值放到一条**undo日志**中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被roll_pointer 属性连接成一个链表，我们把这个链表称之为**版本链**，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的事务id。之后会利用这个记录的版本链来控制并发事务访问相同记录时的行为，把这种机制称之为**多版本并发控制**（Multi-Version Concurrency Control，MVCC）。

------

### undo日志

#### undo log 与 回滚

undo log ：是逻辑日志，记录的是每个事务对数据执行的操作，而不是记录的当前数据，需要根据 undo log 逆推出以往事务的数据。

回滚：为了保证事务的原子性，事务执行过程中遇到错误或者在事务执行时手动输入rollback结束当前事务，导致事务只执行到一半，需要撤销刚才事务里的操作，改回原来的样子，这个过程就叫回滚（rollback）。

#### Undo log 的用途

- 保证事务进行rollback时的原子性和一致性，当事务进行回滚的时候可以用undo log的数据进行恢复。

- 用于MVCC快照读的数据，在MVCC多版本控制中，通过读取undo log的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据版本。

#### undo log 的分类

- insert undo log

代表事务在insert新记录时产生的undo log , 只在事务回滚时需要，并且在事务提交后可以被立即丢弃

- update undo log（主要）

事务在进行update或delete时产生的undo log ， 不仅在事务回滚时需要，在快照读时也需要。所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除

这部分后面会详细讲解。

------

### ReadView（读视图）

#### 简述

对于使用 READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的**最新版本（当前读）**；

对于使用 SERIALIZABLE 隔离级别的事务来说，规定使用加锁的方式来访问记录，读取**最新版本（当前读）**；

对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录。也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，只能是**快照读**。

很重要的一点：**需要判断一下版本链中的哪个版本是当前事务可见的**。可以通过 ReadView 来判断，**ReadView 表示当前事务可以看到版本链中的哪个版本**。

------

#### ReadView 的主要属性

- m_ids ：表示生成 ReadView 时当前系统中活跃的读写事务的事务id 列表（未提交的事务集合，当前事务也在其中）。
- min_trx_id ：表示生成 ReadView 时当前系统中活跃的读写事务中最小的事务id ，也就是m_ids 中的最小值（已提交的事务集合）。
- max_trx_id ：表示生成 ReadView 时系统中应该分配给**下一个事务**的id 值，m_ids 中的最大值加 1（未开始事务）。
- creator_trx_id ：表示生成该 ReadView 的事务的事务id ，就是判断该 id 的事务能读到什么数据。

需要注意

- max_trx_id 并不是 m_ids 中的最大值，事务id是递增分配的。比方说现在有id为1，2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，m_ids就包括1和2，min_trx_id的值就是1，max_trx_id的值就是4。
- 只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。

------

#### 判断记录版本是否可见

有了ReadView ，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：

- 如果被访问版本的trx_id 属性值与ReadView 中的creator_trx_id 值**相同**，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
- 如果被访问版本的trx_id 属性值**小于**ReadView 中的min_trx_id 值，表明生成该版本的事务在当前事务生成ReadView 前已经提交，所以该版本可以被当前事务访问。
- 如果被访问版本的trx_id 属性值**大于**ReadView 中的max_trx_id 值，表明生成该版本的事务在当前事务生成ReadView 后才开启，所以该版本不可以被当前事务访问。
- 如果被访问版本的trx_id 属性值在ReadView 的min_trx_id 和max_trx_id 之间，那就需要判断一下 trx_id 属性值是不是在m_ids 列表中。
  - 如果在，说明创建ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；
  - 如果不在，说明创建ReadView 时生成该版本的事务已经被提交，该版本可以被访问。

如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。

------

#### RR与RC 隔离级别下生成ReadView的时机

Read View主要用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

在MySQL 中， READ COMMITTED 和REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生ReadView的时机不同。

- **READ COMMITTED —— 每次读取数据前都生成一个最新的ReadView**

> 举例：
>
> ![image-20211102191530866](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/106.png)
>
> SELECT1 生成的 ReadView 的 m_ids列表内容为 [100, 200]；SELECT2 生成的 ReadView 的 m_ids列表内容为 [200]。
>
> 若执行的都是：`SELECT * FROM hero WHERE number = 1;`
>
> select1（事务100、200都未提交）：得到的结果是：**刘备**；
>
> select2 （事务100提交、200未提交）：得到的结果是：**张飞**。
>

- **REPEATABLE READ —— 同一事务中，在第一次读取数据时生成一个ReadView**

对于使用REPEATABLE READ 隔离级别的事务来说，只会在第一次执行查询语句时生成一个ReadView ，之后的查询就不会重复生成。

> 举例：
>
> ![image-20211102191921290](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/107.png)
>
> SELECT1 生成的 ReadView 的 m_ids列表内容为 [100, 200]；执行SELECT1 时已经生成过ReadView ，所以SELECT2 直接复用之前的ReadView，此时 m_ids列表内容依旧为 [100，200]。
>
> 若执行 `SELECT * FROM hero WHERE number = 1;`
>
> select1（事务100、200都未提交）：得到的结果是：**刘备**；
>
> select2 （事务100提交、200未提交）：得到的结果是：**刘备**。
>

------

#### RR的快照读语法区别

在可重复读隔离（RR）级别，事务 T 启动的时候会创建一个视图 ReadView，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 `start transaction with consistent snapshot` 这个命令。

- 第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；

- 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。

update 语句本身就是一个事务，语句完成的时候会自动提交。**update是当前读，select是一致性读**。

在读提交隔离（RC）级别下， `start transaction with consistent snapshot`  这个用法没有意义，等效于普通的 start transaction。

------

#### RC、RR级别下的InnoDB快照读区别

- RR级别下，事务的对某条记录的第一次快照读时，生成一个Read View。Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的，而早于Read View创建的事务所做的修改均是可见

- RC级别下的，事务中，每次快照读都会新生成一个Read View, 这就是在RC级别下的事务中可以看到别的事务提交的更新的原因

------

#### 解决幻读问题

快照读：通过MVCC来进行控制的，不用加锁。按照MVCC中规定的“语法”进行增删改查等操作，可以有效避免幻读，不能完全解决幻读。

当前读：通过next-key锁（行锁+gap锁）来解决问题的。

#### MVCC解决幻读失效

> 举例：RR级别下
>
> ```sql
> CREATE TABLE `t` (
> 	`id` int(11) NOT NULL,
> 	`k` int(11) DEFAULT NULL,
> 	PRIMARY KEY (`id`)
> ) ENGINE=InnoDB;
> insert into t(id, k) values(1,1),(2,2);
> ```
>
> ![img](https://static001.geekbang.org/resource/image/82/d6/823acf76e53c0bdba7beab45e72e90d6.png)
>
> begin/start transaction：一致性视图是在执行第一个快照读语句时创建的；
>
> start transaction with consistent snapshot：一致性视图是在执行 start transaction with consistent snapshot 时创建的。
>
> 在这个例子中，事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。事务 B 在更新了行之后查询 ; 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。
>
> 最后，事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1。

**RR级别下，若是T1事务先执行一次select语句生成了一个ReadView，然后T2事务向表里插入了一条新记录，然后T1使用了update语句或者delete语句改动了这条新记录，那么这条数据记录里的trx_id隐藏列的值就会变成改动这个记录的事务id。之后T1再使用select语句之后，读的就是最新值了，还是产生了幻读**。

------

### 小结

MVCC：生成一个ReadView ，然后通过ReadView 找到符合条件的记录版本（历史版本是由undo日志构建的）。普通的 **select 查询语句**只能读到在生成 **ReadView 之前已提交事务**所做的更改，而**update、delete、insert写操作**针对的是**最新版本的记录**，读记录的历史版本和改动记录的最新版本本身并不冲突，也就是采用MVCC 时， 读-写操作并不冲突。

- 对于可重复读，查询只承认在**事务启动前**就已经提交完成的数据；

- 对于读提交，查询只承认在**语句启动前**就已经提交完成的数据；

- 更新语句会采用**当前读**，而当前读，总是读取已经提交完成的最新版本。

参考：[全网最全的一篇数据库MVCC详解，不全我负责-mysql教程-PHP中文网](https://www.php.cn/mysql-tutorials-460111.html)

------

------

# 锁机制

## 基本介绍

### 锁机制的概念

锁机制：锁是数据库用以协调多个进程间并发访问同一共享资源的一种机制。MySQL中为了保证数据访问的一致性与有效性等功能，实现了锁机制，MySQL中的锁是在服务器层或者存储引擎层实现的。

优缺点：锁机制类似于多线程中的同步，可以保证数据的一致性和安全性；但加锁是消耗资源的，锁的各种操作，包括获得锁、检测锁是否已解除、释放锁等 ，都会增加系统的开销；而且锁冲突也会影响数据库并发访问性能。

一个事务T1想对一条记录做改动时，首先会看看内存中有没有与这条记录关联的锁结构，如果没有就会在内存中生成一个锁结构与之关联。

![image-20211104140745340](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/108.png)

其实在锁结构里有很多信息，这里为了简化理解：

- trx信息：代表这个锁结构是哪个事务生成的。
- is_waiting ：代表当前事务是否在等待。false表示获取锁、加锁成功；true表示获取锁失败、加锁失败，需要等待前面的事务提交。

### 锁的分类

- 按操作分类：
  - 共享锁（shared lock）：也叫读锁（read lock）。对同一份数据，多个事务读操作可以同时加锁而不互相影响 ，但不能进行写数据
  - 排他锁（exclusive lock）：也叫写锁（write lock）。当前的操作没有完成前，会阻断其他操作的读取和写入
  - 意向共享锁（IS）：一个事务给一个数据行(一条记录)加共享锁时，必须先获得表的IS锁
  - 意向排它锁（IX）：一个事务给一个数据行(一条记录)加排他锁时，必须先获得该表的IX锁

共享锁与排他锁是行级别锁，意向共享锁与意向排他锁是表级别锁。

- 按粒度分类：
  - 表级锁：会锁定整个表，开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低，偏向 MyISAM
  - 行级锁：会锁定当前操作行，开销大，加锁慢；会出现死锁；锁定力度小，发生锁冲突概率低，并发度高，偏向 InnoDB
  - 页级锁：锁的力度、发生冲突的概率和加锁开销介于表锁和行锁之间，会出现死锁，并发性能一般
- 按使用方式分类：
  - 悲观锁：每次查询数据时都认为别人会修改，很悲观，所以查询时加锁
  - 乐观锁：每次查询数据时都认为别人不会修改，很乐观，但是更新时会判断一下在此期间别人有没有去更新这个数据

IS与IX是互相兼容的，但IS与X、IX与S依旧满足读写锁的相冲突性。

---

## 并发事务带来的问题

### 写-写 的情况

造成更新丢失（脏写）现象，任何一种隔离级别下都不允许产生这种现象。

解决方法：

让未提交事务变成串行操作，而不是并发的操作，通过对读取的记录加排他锁实现

### 读-写 或 写-读 的情况

- 脏读：当前事务读取了另一个未提交事务写的一条记录

- 不可重复读：当前事务先读取一条记录且未提交，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值
- 幻读：当前事务读取了一个范围内的记录且未提交，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记

解决方法：

- 读操作利用多版本并发控制（ MVCC ），写操作进行加锁。

RC级别下，一个事务中每次执行 SELECT 操作都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了**脏读**现象；

RR级别下，一个事务中只有第一次执行SELECT 操作才会生成一个 ReadView ，之后的 SELECT 操作都复用这个 ReadView，这样也就避免了**不可重复读**和**幻读**的问题。

- 读-写操作都采用加锁的方式

通过加锁，不允许读取记录的旧版本，每次都必须去读取记录的最新版本。

脏读：如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录，所以也就不会有脏读问题的产生。

不可重复读：如果在当前事务读取记录时给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读。

幻读：可以通过加不同的锁（record key、gap key、next-key）来防止范围内的数据插入。

采用MVCC 方式的话， 读-写操作彼此并不冲突，性能更高，采用加锁方式的话， 读-写操作彼此需要排队执行，影响性能。

------

## InnoDB行锁与表锁

针对记录的锁，称为行级锁或者**行锁**，行锁的粒度比较细；针对表的加锁，称为表级锁或者**表锁**或多粒度锁。对一个表加锁，锁的是整个表中的记录，表锁的粒度比较粗。

针对表或者行，执行写操作，InnoDB会自动给涉及的数据集加上排他锁；

```sql
-- 写操作自动上锁
insert;  update;  delete;
```

隐式上锁，默认自动加锁自动释放。

### 表锁

- 表级别的S锁、X锁

```sql
 -- 给表加S锁
 lock tables … read; 
 -- 给表加X锁
 lock tables … write;
 
 -- 释放锁
 unlock tables;
```

除了用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。

- 元数据锁（Metadata Locks ，MDL）

用途：防止DDL和DML语句并发的冲突。

元数据锁是**server层**的锁，MDL 不需要显式使用，开启事务，访问一个表的时候会被**自动加上**。

当对一个表做**增删改查**操作的时候，加 **MDL 读锁**；当要对**表做结构变更**操作的时候，加 **MDL 写锁**。读读之间不互斥、读写之间互斥。

- 表级别的IS锁、IX锁

由于表锁和行锁虽然锁定范围不同，但是会相互冲突。当你要加表锁时，势必要先遍历该表的所有记录，判断是否有排他锁。这种遍历检查的方式显然是一种低效的方式，MySQL引入了意向锁，来检测表锁和行锁的冲突。

当事务准备在**某条记录**上加S锁、X锁时，需要先在**表级别**加一个IS、IX锁。

- 表级别的AUTO-INC锁

系统实现这种自动给 AUTO_INCREMENT 修饰的列递增赋值两种方式：

采用AUTO-INC 锁，也就是在执行插入语句时就在表级别加一个AUTO-INC 锁，然后为每条待插入记录的AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC 锁释放掉。

采用一个轻量级的锁，在为插入语句生成AUTO_INCREMENT 修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的AUTO_INCREMENT 列的值之后，就把该轻量级锁释放掉，并不需要等到整个插入语句执行完才释放锁。

### 行锁

#### 行锁分类

**行锁是加在索引上的**，如果当你的查询语句不走索引的话，那么它就会升级到表锁，最终造成效率低下，所以在写SQL语句时需要特别注意。

- 行级别的S锁、X锁

对于写操作 UPDATE、DELETE和INSERT 语句，InnoDB会自动给涉及的数据集加上排他锁。普通的SELECT语句，InnoDB不会加任何锁，但可以显式加锁：

```sql
-- 对读取的记录加S锁
SELECT ... LOCK IN SHARE MODE;
```

此时其他事务仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。

```sql
-- 对读取的记录加X锁
SELECT ... FOR UPDATE;
```

其他事务可以查询记录，但是不能对该记录加共享锁或排他锁，只能等待锁释放后在加锁。

- 两阶段锁协议

行锁，也称为记录锁，就是在记录上加的锁。在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

- 记录锁（Record key）

只一条记录锁（索引）上进行加锁，也就是一行记录，称Record key 。

- 间隙锁（Gap Lock）（RR级别下才有）

当使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“ **间隙（GAP）** ”。因为这些不存在的记录，无法对他们加Record-key，所以才引入了间隙锁，也就是说gap锁是为了防止插入幻影记录而提出的。

在 RU 和 RC 两种隔离级别下，即使使用 select in share mode 或 select for update，也无法防止**幻读**（读后写的场景）。因为这两种隔离级别下只会有**行锁**，而不会有**间隙锁**。而如果是 **RR 隔离级别**的话，就会在间隙上加上间隙锁。

通过 **Infimum 记录**（表示该页面中最小的记录 ）与 **Supremum 记录**（表示该页面中最大的记录）。给Supremum 记录添加间隙锁，来阻止最后一条记录到无穷大的范围插入新纪录。

- 临建锁（Next-key Lock）

既锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录，于是有了临建锁。

临键锁是记录锁与与间隙锁的结合，它既能保护该条记录，又能阻止别的事务将新记录插入被保护记录前边的间隙，并且临键锁是个**左开右闭**的区间。

在RR这种级别下，如果使用 select in share mode 或者 select for update 语句，那么InnoDB会使用临键锁（记录锁 + 间隙锁），因而可以防止幻读，但不能完全防止。

两个优化原则：1.索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁；2. 索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。

- 插入意向锁（Insert Intention Locks）

事务在等待的时候也需要在内存中生成一个锁结构，表明有事务想在某个间隙中插入新记录，但是现在在等待。

> 举例：
>
> ![image-20211104140745340](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/109.png)
>
> 
>
> 从图中可以看到，由于T1 持有gap锁，所以T2 和T3 需要生成一个插入意向锁的锁结构并且处于等待状态。事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁，比较鸡肋。

- 隐式锁

一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于事务id 的存在，相当于加了一个隐式锁。别的事务在对这条记录加S锁或者X锁时，由于隐式锁的存在，会先帮助当前事务生成一个锁结构，然后自己在生成一个锁结构后进入等待状态。

对于聚簇索引记录来说，有一个trx_id 隐藏列，该隐藏列记录着最后改动该记录的事务id 。会看一下该记录的trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个X锁，然后自己进入等待状态。

对于二级索引记录来说，本身并没有trx_id 隐藏列，但是在二级索引页面的Page Header 部分有一个PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的事务id。如果 PAGE_MAX_TRX_ID 属性值小于当前最小的活跃事务id ，那么说明对该页面做修改的事务都已经提交，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复关于聚簇索引的做法。

#### 不同类型锁之间的兼容

不同类型的锁之间的兼容如下表所示：

|          | RECORED | GAP  | NEXT-KEY | II GAP（插入意向锁） |
| -------- | ------- | ---- | -------- | -------------------- |
| RECORED  |         | 兼容 |          | 兼容                 |
| GAP      | 兼容    | 兼容 | 兼容     | 兼容                 |
| NEXT-KEY |         | 兼容 |          | 兼容                 |
| II GAP   | 兼容    |      |          | 兼容                 |

（其中行表示已有的锁，列表示意图加上的锁）

其中，第一行表示已有的锁，第一列表示要加的锁。插入意向锁较为特殊，所以我们先对插入意向锁做个总结，如下：

- 插入意向锁不影响其他事务加其他任何锁。也就是说，一个事务已经获取了插入意向锁，对其他事务是没有任何影响的；
- 插入意向锁与间隙锁和 Next-key 锁冲突。也就是说，一个事务想要获取插入意向锁，如果有其他事务已经加了间隙锁或 Next-key 锁，则会阻塞。

其他类型的锁的规则较为简单：

- 间隙锁不和其他锁（不包括插入意向锁）冲突；
- 记录锁和记录锁冲突，Next-key 锁和 Next-key 锁冲突，记录锁和 Next-key 锁冲突；

![](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/110)

------

## 行锁与表锁的比较

表锁：加锁过程的开销小，加锁的速度快；不会出现死锁的情况；锁定的粒度大，发生锁冲突的几率大，并发度低。

- 一般在执行DDL语句时会对整个表进行加锁，比如说 ALTER TABLE 等操作；
- 如果对InnoDB的表使用行锁，被锁定字段不是主键，也没有针对它建立索引的话，那么将会锁整张表；
- 表级锁更适合于以查询为主，并发用户少，只有少量按索引条件更新数据的应用，如Web 应用。

行锁：加锁过程的开销大，加锁的速度慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

- 最大程度的支持并发，同时也带来了最大的锁开销。
- 在 InnoDB 中，除单个 SQL 组成的事务外，锁是逐步获得的，这就决定了在 InnoDB 中发生死锁是可能的。
- 行级锁只在存储引擎层实现，而 MySQL 服务器层没有实现。 行级锁更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。

从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如 Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理系统

------

## 不同存储引擎支持的锁

| 存储引擎 | 表级锁   | 行级锁   | 页级锁 |
| -------- | -------- | -------- | ------ |
| MyISAM   | 支持     | 不支持   | 不支持 |
| InnoDB   | **支持** | **支持** | 不支持 |
| MEMORY   | 支持     | 不支持   | 不支持 |
| BDB      | 支持     | 不支持   | 支持   |

InnoDB默认行锁（存储引擎实现），MyISAM默认表锁（Server层实现）。

------

## InnoDB锁的内存结构

对一条记录加锁的本质就是在内存中创建一个锁结构与之关联。

![image-20211117173110588](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/111.png)

- 锁所在的事务信息

相当于一个指针，指向生成了这个锁结构的事务，记载着事务id等信息。

- 索引信息

对于行锁来说，需要记录一下加锁的记录是属于哪个索引的。

- 表锁／行锁信息

  - 表锁：
    记载着这是对哪个表加的锁，还有其他的一些信息。

  - 行锁：

    Space ID ：记录所在表空间。

    Page Number ：记录所在页号。

    n_bits ：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同的比特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bits 属性代表使用了多少比特位。

- type_mode 

这是一个32位的数，被分成了lock_mode 、lock_type 和rec_lock_type 三个部分，如图所示：

![image-20211104161825995](https://github.com/Frank-gg/MySQL-notes/blob/master/MySQL%20Picture/112.png)

**锁的模式（ lock_mode ）**，占用低4位：

​		LOCK_IS （十进制的0 ）、LOCK_IX （1）、LOCK_S （2）、LOCK_X （3）、LOCK_AUTO_INC （4）。

​		在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。

**锁的类型（ lock_type ）**，占用第5～8位：

​		LOCK_TABLE （十进制的16 ），也就是当第5个比特位置为1时，表示表级锁。LOCK_REC （十进制的32 ），也就是当第6个比特位置为1时，表示行级锁。

​		行锁的具体类型（ rec_lock_type ），使用其余的位来表示。只有在lock_type 的值为LOCK_REC 时，也就是只有在该锁为行级锁时，才会被细分为更多的类型。

**LOCK_ORDINARY （十进制的0 ）**：

​		表示next-key锁。LOCK_GAP （十进制的512 ）：也就是当第10个比特位置为1时，表示gap锁。LOCK_REC_NOT_GAP （十进制的1024 ）：也就是当第11个比特位置为1时，表示正经记录锁。LOCK_INSERT_INTENTION （十进制的2048 ）：也就是当第12个比特位置为1时，表示插入意向
锁。

------

## 死锁

**死锁**：两个或多个事务互相等待对方先释放掉与自己需要的锁相冲突的锁，导致两个事务都不能继续执行。

> 举例：A事务持有x1锁 ，申请x2锁，B事务持有x2锁，申请x1 锁。A和B事务持有锁并且申请对方持有的锁进入循环等待，就造成死锁。

### 死锁产生条件

- 两个或者两个以上事务。
- 每个事务都已经持有锁并且申请新的锁。
- 锁资源同时只能被同一个事务持有或者不兼容。
- 事务之间因为持有锁和申请锁导致了循环等待

### 死锁处理

- 超时死锁检测

直接进入等待直到超时，超时后自动回滚代价较小的事务（innodb_lock_wait_timeout 默认50s），并向客户端发送一条信息。

```sql
Error 1212 (40001):Deadlock found when trying to get lock;try restarting transaction
```

但是时间的设置不好控制，超时可能不是因为死锁，而是因为事务处理比较慢，所以一般不采取该方式

- 定位死锁

```sql
-- 查看死锁日志
show engine innodb status \G
```

然后查看`LATEST DETECTED DEADLOCK`，查找最近一次死锁信息。

`show engine innodb status`只显示最近一次死锁的信息。若要全部信息都记录在错误日志中，可以把全局系统变量 `innodb_print_all_deadlocks`设置为ON。

- 人为解决，kill阻塞进程（show processlist）

- wait for graph 等待图（主动检测）：把事务等待列表和锁等待信息列表通过事务信息进行wait-for graph检测，如果发现有闭环，则回滚undo log量少的事务。

### 死锁避免

- 加锁顺序一致，尽可能一次性锁定所需的数据行
- 尽量基于primary（主键）或unique key更新数据
- 单次操作数据量不宜过多，涉及表尽量少
- 减少表上索引，减少锁定资源
- 尽量使用较低的隔离级别
- 尽量使用相同条件访问数据，这样可以避免间隙锁对并发的插入影响
- 精心设计索引，尽量使用索引访问数据
- 借助相关工具：pt-deadlock-logger

参考：

[【MySQL】MySQL中的锁机制 - 周二鸭 - 博客园 (cnblogs.com)](https://www.cnblogs.com/jojop/p/13982679.html)

[一张图彻底搞懂 MySQL 的锁机制 | MySQL 技术论坛 (learnku.com)](https://learnku.com/articles/39212)

------

------

